{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2231f126-c2f0-4363-8e37-7bfeb0fe41cd",
      "metadata": {
        "id": "2231f126-c2f0-4363-8e37-7bfeb0fe41cd"
      },
      "source": [
        "## Implementing the Data Preprocessing (Dataset Partitioning) and Model Training for the baseline Model Confidence Based Exclusion (MCE) defense described in the MIAShield paper.\n",
        "* The target dataset is CIFAR-10, and the target model architecture is AlexNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8f2b601e-46b6-413e-b4ec-f8253879ccd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f2b601e-46b6-413e-b4ec-f8253879ccd2",
        "outputId": "ee66e8d4-ef5c-4d6b-b782-d0afd946b8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "#imports and setup\n",
        "\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split, Subset\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d77963fa-511c-422b-805b-bdcb1ca69ec2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d77963fa-511c-422b-805b-bdcb1ca69ec2",
        "outputId": "cacdf92d-e5b3-416e-807b-6e9d962793a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45b68eb-be91-4e28-86f0-1cfaff3e9e2d",
      "metadata": {
        "id": "b45b68eb-be91-4e28-86f0-1cfaff3e9e2d"
      },
      "source": [
        "## From the paper, original CIFAR-10 dataset has 50,000 training images (Dtrain) and 10,000 test images (Dtest)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d4fc9cf9-585d-4dc0-a0eb-855c8857a8ba",
      "metadata": {
        "id": "d4fc9cf9-585d-4dc0-a0eb-855c8857a8ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d7be59-66ea-4578-e2ee-f6aa36b8c53c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Define the transformations (normalization is key for training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Normalization parameters for CIFAR-10\n",
        "    # mean for CIFAR-10 = (0.4914, 0.4822, 0.4465)\n",
        "    #std for CIFAR-10  = (0.2023, 0.1994, 0.2010)\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Load original datasets\n",
        "original_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "original_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1PcPadeTbspo",
      "metadata": {
        "id": "1PcPadeTbspo"
      },
      "source": [
        "## Reproducibility, AlexNet (CIFAR-10), and transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c0473744-2170-4d09-b922-d226de729ff5",
      "metadata": {
        "id": "c0473744-2170-4d09-b922-d226de729ff5"
      },
      "outputs": [],
      "source": [
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ---- AlexNet variant for CIFAR-10 (32x32) ----\n",
        "class CIFARAlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # 32x32 -> 32x32\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # 32x32 -> 16x16\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # 16x16 -> 16x16\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # 16x16 -> 8x8\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # 8x8 -> 8x8\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # 8x8 -> 8x8\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # 8x8 -> 8x8\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # 8x8 -> 4x4\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # 256 * 4 * 4 = 4096\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256*4*4, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)  # (N, 256*4*4)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Augmentation for training (paper §6.3): flip, ±10° rotation, ±10% translate, ~0.2% zoom\n",
        "train_transform_w_aug = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.10, 0.10), scale=(0.998, 1.002)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# No-aug (for eval and loaders that shouldn't augment)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uQOr4zJScnJD",
      "metadata": {
        "id": "uQOr4zJScnJD"
      },
      "source": [
        "## Data Partition CIFAR-10 exactly as in Table 1 (n=5) and build EO/MIAShield splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "rm2ovQjnciYf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm2ovQjnciYf",
        "outputId": "9ccf4b3b-1cdf-4169-9a78-2aa2445e4f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Dataset Partition Summary =====\n",
            "Total CIFAR-10 Train Samples: 50000\n",
            "Total CIFAR-10 Test Samples:  10000\n",
            "\n",
            "Subset Dtrain_1: 10000 samples\n",
            "Subset Dtrain_2: 10000 samples\n",
            "Subset Dtrain_3: 10000 samples\n",
            "Subset Dtrain_4: 10000 samples\n",
            "Subset Dtrain_5: 10000 samples\n",
            "\n",
            "--- Exclusion Oracle (EO) Train Set ---\n",
            "Members (from train):      12500\n",
            "Non-members (from test):   5000\n",
            "\n",
            "--- MIAShield Test Set ---\n",
            "Members (from train):      5000\n",
            "Non-members (from test):   5000\n"
          ]
        }
      ],
      "source": [
        "# Use the already-downloaded datasets but re-wrap them with the correct transforms when needed\n",
        "full_train_eval = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=eval_transform)\n",
        "full_test_eval  = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=eval_transform)\n",
        "\n",
        "NUM_TRAIN = len(full_train_eval)   # 50_000\n",
        "NUM_TEST  = len(full_test_eval)    # 10_000\n",
        "n = 5  # number of disjoint subsets (CIFAR-10)\n",
        "\n",
        "# ---- Disjoint split of the training set into n equal parts ----\n",
        "all_train_idx = np.arange(NUM_TRAIN)\n",
        "np.random.shuffle(all_train_idx)\n",
        "splits = np.array_split(all_train_idx, n)  # list of 5 arrays (~10k each)\n",
        "\n",
        "# ---- EO train set: 2.5k x n members (from train) + 5k non-members (from test) ----\n",
        "EO_MEM_PER_SPLIT = 2500\n",
        "eo_mem_indices = []\n",
        "for s in splits:\n",
        "    eo_mem_indices.extend(np.random.choice(s, size=EO_MEM_PER_SPLIT, replace=False))\n",
        "eo_mem_indices = np.array(eo_mem_indices)  # length = 12_500\n",
        "\n",
        "eo_nonmem_indices = np.random.choice(np.arange(NUM_TEST), size=5000, replace=False)\n",
        "\n",
        "Dtrain_EO_members     = Subset(full_train_eval, eo_mem_indices.tolist())\n",
        "Dtrain_EO_nonmembers  = Subset(full_test_eval,  eo_nonmem_indices.tolist())\n",
        "\n",
        "# ---- MIAShield test set: 5k members (from train) + 5k non-members (from test)\n",
        "# ensure disjointness with Dtrain_EO to avoid bias/leakage\n",
        "remaining_train = np.setdiff1d(all_train_idx, eo_mem_indices, assume_unique=False)\n",
        "remaining_test  = np.setdiff1d(np.arange(NUM_TEST), eo_nonmem_indices, assume_unique=False)\n",
        "test_mem_indices    = np.random.choice(remaining_train, size=5000, replace=False)\n",
        "test_nonmem_indices = np.random.choice(remaining_test,  size=5000, replace=False)\n",
        "\n",
        "Dtest_MIASHIELD_members    = Subset(full_train_eval, test_mem_indices.tolist())\n",
        "Dtest_MIASHIELD_nonmembers = Subset(full_test_eval,  test_nonmem_indices.tolist())\n",
        "\n",
        "# === Summary of Dataset Partitioning (CIFAR-10) ===\n",
        "print(\"===== Dataset Partition Summary =====\")\n",
        "print(f\"Total CIFAR-10 Train Samples: {len(full_train_eval)}\")\n",
        "print(f\"Total CIFAR-10 Test Samples:  {len(full_test_eval)}\\n\")\n",
        "\n",
        "for i, s in enumerate(splits, start=1):\n",
        "    print(f\"Subset Dtrain_{i}: {len(s)} samples\")\n",
        "\n",
        "print(\"\\n--- Exclusion Oracle (EO) Train Set ---\")\n",
        "print(f\"Members (from train):      {len(Dtrain_EO_members)}\")\n",
        "print(f\"Non-members (from test):   {len(Dtrain_EO_nonmembers)}\")\n",
        "\n",
        "print(\"\\n--- MIAShield Test Set ---\")\n",
        "print(f\"Members (from train):      {len(Dtest_MIASHIELD_members)}\")\n",
        "print(f\"Non-members (from test):   {len(Dtest_MIASHIELD_nonmembers)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eTKLU3vJdNeX",
      "metadata": {
        "id": "eTKLU3vJdNeX"
      },
      "source": [
        "## DataLoaders (batch size = 128) for training and evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Sja6CnqcdB5j",
      "metadata": {
        "id": "Sja6CnqcdB5j"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "# For each subset, train loader **with augmentation** and an eval loader **without**.\n",
        "# Build subset Datasets twice: one with aug transform for training, one with eval transform for evaluation.\n",
        "def make_subset_dataset(indices, with_aug: bool):\n",
        "    base = torchvision.datasets.CIFAR10(root='./data', train=True, download=False,\n",
        "                                        transform=train_transform_w_aug if with_aug else eval_transform)\n",
        "    return Subset(base, indices.tolist())\n",
        "\n",
        "subset_train_loaders = []\n",
        "subset_eval_loaders  = []\n",
        "\n",
        "for s in splits:\n",
        "    ds_train = make_subset_dataset(s, with_aug=True)\n",
        "    ds_eval  = make_subset_dataset(s, with_aug=False)\n",
        "    subset_train_loaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True))\n",
        "    subset_eval_loaders.append( DataLoader(ds_eval,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True))\n",
        "\n",
        "# EO & MIAShield loaders (evaluation only — no aug)\n",
        "loader_train_EO_mem    = DataLoader(Dtrain_EO_members,    batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "loader_train_EO_nonmem = DataLoader(Dtrain_EO_nonmembers, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "loader_test_MIASHIELD_mem    = DataLoader(Dtest_MIASHIELD_members,    batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "loader_test_MIASHIELD_nonmem = DataLoader(Dtest_MIASHIELD_nonmembers, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Full test loader for reporting model accuracies (like Table 2)\n",
        "full_test_loader = DataLoader(full_test_eval, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XqwIvigWd4zg",
      "metadata": {
        "id": "XqwIvigWd4zg"
      },
      "source": [
        "## Train loop (Adam Optimizer, Cross Entropy- Loss Function, 60 epochs @ lr=0.01) and evaluation helpers + weight init helper to assist with training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b1rVe7kweN7U",
      "metadata": {
        "id": "b1rVe7kweN7U"
      },
      "outputs": [],
      "source": [
        "# ---- weight init helper (optional) ----\n",
        "def kaiming_init(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "def build_model():\n",
        "    net = CIFARAlexNet(num_classes=10)\n",
        "    net.apply(kaiming_init)\n",
        "    return net\n",
        "\n",
        "def train_one_model(model, train_loader, epochs=60, device=device):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # ↓↓↓ KEY CHANGES ↓↓↓\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 45, 55], gamma=0.1)\n",
        "    # ↑↑↑ KEY CHANGES ↑↑↑\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running += loss.item() * xb.size(0)\n",
        "        scheduler.step()\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - loss: {running/len(train_loader.dataset):.4f}\")\n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(model, data_loader, device=device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for xb, yb in data_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return 100.0 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "buh-bR3TfIuO",
      "metadata": {
        "id": "buh-bR3TfIuO"
      },
      "source": [
        "## Train the five-model ensemble on the five disjoint subsets (with augmentation) and report accuracies (Table 2 style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "CsIzzMTKe9mO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsIzzMTKe9mO",
        "outputId": "bbd75ee0-8b3a-4f75-c9d1-54c917b3f1ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model f1 on its disjoint subset (with augmentation)...\n",
            "Epoch 5/60 - loss: 1.7597\n",
            "Epoch 10/60 - loss: 1.3991\n",
            "Epoch 15/60 - loss: 1.2060\n",
            "Epoch 20/60 - loss: 1.0596\n",
            "Epoch 25/60 - loss: 0.9324\n",
            "Epoch 30/60 - loss: 0.8256\n",
            "Epoch 35/60 - loss: 0.6176\n",
            "Epoch 40/60 - loss: 0.5691\n",
            "Epoch 45/60 - loss: 0.5226\n",
            "Epoch 50/60 - loss: 0.4937\n",
            "Epoch 55/60 - loss: 0.4862\n",
            "Epoch 60/60 - loss: 0.4715\n",
            "f1 accuracy on CIFAR-10 test (with aug): 73.59%\n",
            "Saved model as f1.pth\n",
            "\n",
            "Training model f2 on its disjoint subset (with augmentation)...\n",
            "Epoch 5/60 - loss: 1.6645\n",
            "Epoch 10/60 - loss: 1.3652\n",
            "Epoch 15/60 - loss: 1.1896\n",
            "Epoch 20/60 - loss: 1.0478\n",
            "Epoch 25/60 - loss: 0.9410\n",
            "Epoch 30/60 - loss: 0.8215\n",
            "Epoch 35/60 - loss: 0.6033\n",
            "Epoch 40/60 - loss: 0.5554\n",
            "Epoch 45/60 - loss: 0.5113\n",
            "Epoch 50/60 - loss: 0.4750\n",
            "Epoch 55/60 - loss: 0.4731\n",
            "Epoch 60/60 - loss: 0.4638\n",
            "f2 accuracy on CIFAR-10 test (with aug): 73.53%\n",
            "Saved model as f2.pth\n",
            "\n",
            "Training model f3 on its disjoint subset (with augmentation)...\n",
            "Epoch 5/60 - loss: 1.6357\n",
            "Epoch 10/60 - loss: 1.3417\n",
            "Epoch 15/60 - loss: 1.1523\n",
            "Epoch 20/60 - loss: 1.0022\n",
            "Epoch 25/60 - loss: 0.9229\n",
            "Epoch 30/60 - loss: 0.8256\n",
            "Epoch 35/60 - loss: 0.5995\n",
            "Epoch 40/60 - loss: 0.5388\n",
            "Epoch 45/60 - loss: 0.5103\n",
            "Epoch 50/60 - loss: 0.4633\n",
            "Epoch 55/60 - loss: 0.4672\n",
            "Epoch 60/60 - loss: 0.4532\n",
            "f3 accuracy on CIFAR-10 test (with aug): 74.19%\n",
            "Saved model as f3.pth\n",
            "\n",
            "Training model f4 on its disjoint subset (with augmentation)...\n",
            "Epoch 5/60 - loss: 1.5888\n",
            "Epoch 10/60 - loss: 1.3083\n",
            "Epoch 15/60 - loss: 1.1017\n",
            "Epoch 20/60 - loss: 0.9565\n",
            "Epoch 25/60 - loss: 0.8522\n",
            "Epoch 30/60 - loss: 0.7403\n",
            "Epoch 35/60 - loss: 0.5140\n",
            "Epoch 40/60 - loss: 0.4780\n",
            "Epoch 45/60 - loss: 0.4415\n",
            "Epoch 50/60 - loss: 0.4004\n",
            "Epoch 55/60 - loss: 0.3933\n",
            "Epoch 60/60 - loss: 0.3977\n",
            "f4 accuracy on CIFAR-10 test (with aug): 74.92%\n",
            "Saved model as f4.pth\n",
            "\n",
            "Training model f5 on its disjoint subset (with augmentation)...\n",
            "Epoch 5/60 - loss: 1.7054\n",
            "Epoch 10/60 - loss: 1.3872\n",
            "Epoch 15/60 - loss: 1.1908\n",
            "Epoch 20/60 - loss: 1.0493\n",
            "Epoch 25/60 - loss: 0.9325\n",
            "Epoch 30/60 - loss: 0.8088\n",
            "Epoch 35/60 - loss: 0.5807\n",
            "Epoch 40/60 - loss: 0.5362\n",
            "Epoch 45/60 - loss: 0.4954\n",
            "Epoch 50/60 - loss: 0.4662\n",
            "Epoch 55/60 - loss: 0.4527\n",
            "Epoch 60/60 - loss: 0.4585\n",
            "f5 accuracy on CIFAR-10 test (with aug): 75.21%\n",
            "Saved model as f5.pth\n"
          ]
        }
      ],
      "source": [
        "models = []\n",
        "acc_table = []\n",
        "for i, tr_loader in enumerate(subset_train_loaders, start=1):\n",
        "    print(f\"\\nTraining model f{i} on its disjoint subset (with augmentation)...\")\n",
        "    net = build_model()\n",
        "    net = train_one_model(net, tr_loader, epochs=60)\n",
        "\n",
        "    # Store in-memory\n",
        "    models.append(net)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    acc = accuracy(net, full_test_loader)\n",
        "    acc_table.append((f\"f{i}\", acc))\n",
        "    print(f\"f{i} accuracy on CIFAR-10 test (with aug): {acc:.2f}%\")\n",
        "\n",
        "     # ---- SAVE TO DISK ----\n",
        "    torch.save(net.state_dict(), f\"f{i}.pth\")\n",
        "    print(f\"Saved model as f{i}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "TxtgQFpHXHJ6",
      "metadata": {
        "id": "TxtgQFpHXHJ6"
      },
      "outputs": [],
      "source": [
        "## If you want to download it to your system - Run the 2 code block below\n",
        "## Or Skip and Load it from Step 2\n",
        "\n",
        "#import zipfile\n",
        "\n",
        "#model_files = [\"f1.pth\", \"f2.pth\", \"f3.pth\", \"f4.pth\", \"f5.pth\"]\n",
        "\n",
        "#with zipfile.ZipFile(\"models_f1_f5.zip\", 'w') as zipf:\n",
        " #   for file in model_files:\n",
        "  #      zipf.write(file)\n",
        "\n",
        "#print(\"Zip created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "XNBcDRwzcira",
      "metadata": {
        "id": "XNBcDRwzcira"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#files.download(\"models_f1_f5.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gDqeLLlbdGOx",
      "metadata": {
        "id": "gDqeLLlbdGOx"
      },
      "source": [
        "## For Step 2\n",
        "\n",
        "Load the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3L3cgsnWdgRQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L3cgsnWdgRQ",
        "outputId": "2c3a5584-b682-43ab-f6b7-8bd121df3be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 test accuracy: 73.59\n"
          ]
        }
      ],
      "source": [
        "def load_model(path):\n",
        "    model = build_model().to(device)\n",
        "    state_dict = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Load the five models\n",
        "f1 = load_model(\"f1.pth\")\n",
        "f2 = load_model(\"f2.pth\")\n",
        "f3 = load_model(\"f3.pth\")\n",
        "f4 = load_model(\"f4.pth\")\n",
        "f5 = load_model(\"f5.pth\")\n",
        "\n",
        "models = [f1, f2, f3, f4, f5]\n",
        "\n",
        "# (Optional) double-check f1 accuracy achieved in the setup\n",
        "print(\"f1 test accuracy:\", accuracy(f1, full_test_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08d885d",
      "metadata": {
        "id": "c08d885d"
      },
      "source": [
        "APPLYING MCE TO THE CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6db86f2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6db86f2a",
        "outputId": "1b608aef-e802-4299-fd56-7efaf2a6d81a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating MCE Oracle on 10000 test samples...\n",
            "\n",
            "MCE Oracle Accuracy on full test set: 78.78%\n",
            "Model Exclusion Counts:\n",
            "   • Model 1: excluded 1771 times\n",
            "   • Model 2: excluded 1594 times\n",
            "   • Model 3: excluded 1862 times\n",
            "   • Model 4: excluded 2847 times\n",
            "   • Model 5: excluded 1926 times\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def mce_oracle_predict(x_tensor, ensemble_models, device):\n",
        "    all_predictions = []\n",
        "    for model in ensemble_models:\n",
        "        model.eval() # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            output = model(x_tensor.unsqueeze(0).to(device))\n",
        "            # Get probabilities\n",
        "            probabilities = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
        "            all_predictions.append(probabilities)\n",
        "\n",
        "    predictions = np.array(all_predictions) # Shape: (num_models, num_classes)\n",
        "\n",
        "    top_labels = [np.argmax(p) for p in predictions]\n",
        "\n",
        "    # Handle cases where top_labels might be empty\n",
        "    if not top_labels:\n",
        "        return np.zeros(predictions.shape[1]), 0\n",
        "\n",
        "    # Find the majority label among the top predictions of all models\n",
        "    majority_label_counts = Counter(top_labels)\n",
        "    majority_label = majority_label_counts.most_common(1)[0][0]\n",
        "\n",
        "    # Calculate confidence of each model in the majority label\n",
        "    label_confidences = [p[majority_label] for p in predictions]\n",
        "    # Exclude the model with the *lowest* confidence in the majority class\n",
        "    excluded_idx = np.argmax(label_confidences)\n",
        "\n",
        "    # Remove the predictions of the excluded model and average the rest\n",
        "    remaining_preds = np.delete(predictions, excluded_idx, axis=0)\n",
        "    final_pred = np.mean(remaining_preds, axis=0)\n",
        "\n",
        "    return final_pred, excluded_idx\n",
        "\n",
        "def evaluate_mce_on_test(dataset, models, device):\n",
        "    correct = 0\n",
        "    exclusion_counts = np.zeros(len(models), dtype=int)\n",
        "    total_samples = len(dataset)\n",
        "\n",
        "    print(f\"\\nEvaluating MCE Oracle on {total_samples} test samples...\")\n",
        "    for i in range(total_samples):\n",
        "        x_tensor, y_true = dataset[i]\n",
        "\n",
        "        pred_probs, excluded = mce_oracle_predict(x_tensor, models, device)\n",
        "        y_pred = np.argmax(pred_probs)\n",
        "\n",
        "        if y_pred == y_true:\n",
        "            correct += 1\n",
        "\n",
        "        exclusion_counts[excluded] += 1\n",
        "\n",
        "    accuracy = correct / total_samples\n",
        "    print(f\"\\nMCE Oracle Accuracy on full test set: {accuracy * 100:.2f}%\")\n",
        "    print(\"Model Exclusion Counts:\")\n",
        "    for i, count in enumerate(exclusion_counts):\n",
        "        print(f\"   • Model {i+1}: excluded {count} times\")\n",
        "\n",
        "    return accuracy, exclusion_counts\n",
        "\n",
        "\n",
        "acc_mce, exclusion_stats = evaluate_mce_on_test(full_test_eval, models, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# --- 1. Confidence Deviation Exclusion (CDE) ---\n",
        "def confidence_deviation_predict(x_tensor, ensemble_models, device, **kwargs):\n",
        "  \"\"\"\n",
        "  Excludes the model whose confidence on the majority label deviates most\n",
        "  (absolute difference) from the ensemble mean.\n",
        "  \"\"\"\n",
        "  all_probs = []\n",
        "  for model in ensemble_models:\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "      output = model(x_tensor.unsqueeze(0).to(device))\n",
        "      all_probs.append(torch.softmax(output, dim=1).cpu().numpy()[0])\n",
        "  predictions = np.array(all_probs) # Shape: (5, 10)\n",
        "\n",
        "  # Identify Majority Label\n",
        "  top_labels = [np.argmax(p) for p in predictions]\n",
        "  if not top_labels:\n",
        "    return np.zeros(predictions.shape[1]), 0\n",
        "  majority_label = Counter(top_labels).most_common(1)[0][0]\n",
        "\n",
        "  # Calculate Deviation\n",
        "  target_confidence = predictions[:, majority_label]\n",
        "  mean_confidence = np.mean(target_confidence)\n",
        "  deviation = np.abs(target_confidence - mean_confidence)\n",
        "\n",
        "  # Exclude the outlier\n",
        "  excluded_idx = np.argmax(deviation)\n",
        "\n",
        "  remaining_preds = np.delete(predictions, excluded_idx, axis=0)\n",
        "  final_pred = np.mean(remaining_preds, axis=0)\n",
        "\n",
        "  return final_pred, excluded_idx\n",
        "\n",
        "# --- 2. Historical Calibration Error (HCE) Helpers ---\n",
        "def compute_ece(model, loader, device, n_bin=10):\n",
        "  \"\"\"\n",
        "  Calculates expected calibration error (ECE) for a single model.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  bin_boundaries = torch.linspace(0, 1, n_bin + 1)\n",
        "  confidence_list = []\n",
        "  predictions_list = []\n",
        "  labels_list = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for xb, yb in loader:\n",
        "      xb, yb = xb.to(device), yb.to(device)\n",
        "      logits = model(xb)\n",
        "      probs = torch.softmax(logits, dim=1)\n",
        "      conf, preds = torch.max(probs, 1)\n",
        "      confidence_list.append(conf)\n",
        "      predictions_list.append(preds)\n",
        "      labels_list.append(yb)\n",
        "\n",
        "  confidence = torch.cat(confidence_list)\n",
        "  predictions = torch.cat(predictions_list)\n",
        "  labels = torch.cat(labels_list)\n",
        "  accuracies = predictions.eq(labels)\n",
        "\n",
        "  ece = torch.zeros(1, device=device)\n",
        "  for bin_lower, bin_upper in zip(bin_boundaries[:-1], bin_boundaries[1:]):\n",
        "    in_bin = confidence.gt(bin_lower.item()) * confidence.le(bin_upper.item())\n",
        "    prop_in_bin = in_bin.float().mean()\n",
        "    if prop_in_bin.item() > 0:\n",
        "      accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "      avg_confidence_in_bin = confidence[in_bin].mean()\n",
        "      ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "  return ece.item()\n",
        "\n",
        "def calibration_weighted_predict(x_tensor, ensemble_models, device, ece_scores=None):\n",
        "  \"\"\"\n",
        "  Excludes model based on Confidence weighted by historical calibration error.\n",
        "  Score = Confidence * (1 + ECE). Higher score = higher likelihood of being\n",
        "  excluded.\n",
        "  \"\"\"\n",
        "  if ece_scores is None:\n",
        "    raise ValueError(\"ECE scores must be provided.\")\n",
        "\n",
        "  all_probs = []\n",
        "  for model in ensemble_models:\n",
        "    model.eval() # Set model to evaluation\n",
        "    with torch.no_grad():\n",
        "      output = model(x_tensor.unsqueeze(0).to(device))\n",
        "      all_probs.append(torch.softmax(output, dim=1).cpu().numpy()[0])\n",
        "\n",
        "  predictions = np.array(all_probs) # Shape\n",
        "  top_labels = [np.argmax(p) for p in predictions]\n",
        "  majority_label = Counter(top_labels).most_common(1)[0][0]\n",
        "\n",
        "  target_confidence = predictions[:, majority_label]\n",
        "\n",
        "  # Weight the confidence by model's general calibration error\n",
        "  exclusion_scores = []\n",
        "  for i, conf in enumerate(target_confidence):\n",
        "    score = conf * (1.0 + ece_scores[i])\n",
        "    exclusion_scores.append(score)\n",
        "\n",
        "  excluded_idx = np.argmax(exclusion_scores)\n",
        "  remaining_preds = np.delete(predictions, excluded_idx, axis=0)\n",
        "  final_pred = np.mean(remaining_preds, axis=0)\n",
        "  return final_pred, excluded_idx\n",
        "\n",
        "# --- 3. Hybrid Approach (KL Divergence) ---\n",
        "def hybrid_kl_predict(x_tensor, ensemble_models, device, **kwargs):\n",
        "  \"\"\"\n",
        "  Excludes the model whose output distribution diverges most (KL Divergence)\n",
        "  from the consensus distribution of the ensemble.\n",
        "  \"\"\"\n",
        "  all_prods = []\n",
        "  for model in ensemble_models:\n",
        "    model.eval() # Set model to evaluation\n",
        "    with torch.no_grad():\n",
        "      output = model(x_tensor.unsqueeze(0).to(device))\n",
        "      all_prods.append(torch.softmax(output, dim=1).cpu().numpy()[0])\n",
        "\n",
        "  predictions = np.array(all_prods) # Shape\n",
        "\n",
        "  # Calculate Consensus (Mean Distribution)\n",
        "  consensus = np.mean(predictions, axis=0)\n",
        "\n",
        "  # Calculate KL Divergence for each model vs Consensus\n",
        "  # entropy(pk, qk) calculates KL(pk || qk)\n",
        "  kl_divergences = [entropy(pred, consensus) for pred in predictions]\n",
        "\n",
        "  excluded_idx = np.argmax(kl_divergences)\n",
        "  remaining_preds = np.delete(predictions, excluded_idx, axis=0)\n",
        "  final_pred = np.mean(remaining_preds, axis=0)\n",
        "  return final_pred, excluded_idx"
      ],
      "metadata": {
        "id": "wsklgTZKPp5n"
      },
      "id": "wsklgTZKPp5n",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup: Calculate ECE Scores (Required for Strategy 2) ---\n",
        "print(\"Computing ECE scores...\")\n",
        "\n",
        "ece_scores = [compute_ece(m, loader_test_MIASHIELD_nonmem, device) for m in models]\n",
        "print(f\"ECE Scores: {ece_scores}\\n\")\n",
        "\n",
        "# --- Generic Evaluation Runner ---\n",
        "def run_evaluation(strategy, predict_fn, dataset, models, device, **kwargs):\n",
        "  correct = 0\n",
        "  exclusion_counts = np.zeros(len(models), dtype=int)\n",
        "  total_samples = len(dataset)\n",
        "\n",
        "  print(f\"--- Evaluating {strategy} ---\")\n",
        "\n",
        "  for i in range(total_samples):\n",
        "    x_tensor, y_true = dataset[i]\n",
        "\n",
        "    # Execute the specific prediction strategy\n",
        "    pred_probs, excluded = predict_fn(x_tensor, models, device, **kwargs)\n",
        "\n",
        "    y_pred = np.argmax(pred_probs)\n",
        "    if y_pred == y_true:\n",
        "      correct += 1\n",
        "    exclusion_counts[excluded] += 1\n",
        "\n",
        "  accuracy = correct / total_samples\n",
        "  print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "  print(f\"Exclusion Counts: {exclusion_counts.tolist()}\\n\")\n",
        "\n",
        "# --- Run all Evaluations on Full Test Set ---\n",
        "\n",
        "# Confidence Deviation\n",
        "run_evaluation(\"Confidence Deviation\", confidence_deviation_predict, full_test_eval, models, device)\n",
        "\n",
        "# Historical Calibration\n",
        "run_evaluation(\"Historical Calibration\", calibration_weighted_predict, full_test_eval, models, device, ece_scores=ece_scores)\n",
        "\n",
        "# Hybrid (KL Divergence)\n",
        "run_evaluation(\"Hybrid (KL Divergence)\", hybrid_kl_predict, full_test_eval, models, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luGSLzpimsHa",
        "outputId": "61c8829f-2efc-43e3-9096-a6e800d407a2"
      },
      "id": "luGSLzpimsHa",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing ECE scores...\n",
            "ECE Scores: [0.0882967934012413, 0.09203436970710754, 0.08981642127037048, 0.10654717683792114, 0.08498027920722961]\n",
            "\n",
            "--- Evaluating Confidence Deviation ---\n",
            "Accuracy: 78.51%\n",
            "Exclusion Counts: [1993, 2309, 2095, 1790, 1813]\n",
            "\n",
            "--- Evaluating Historical Calibration ---\n",
            "Accuracy: 78.79%\n",
            "Exclusion Counts: [1063, 1286, 1224, 5451, 976]\n",
            "\n",
            "--- Evaluating Hybrid (KL Divergence) ---\n",
            "Accuracy: 78.38%\n",
            "Exclusion Counts: [1927, 2239, 2018, 1942, 1874]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Define transforms for consistency\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Download and prepare CIFAR-10 dataset\n",
        "full_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Let's split the full training set into full_train (used for members) and full_val (optional)\n",
        "full_train, _ = random_split(full_dataset, [50000, 0])  # or just assign directly\n",
        "\n",
        "# This is already defined elsewhere in your code as test set, used for non-members\n",
        "full_test_eval = test_dataset\n"
      ],
      "metadata": {
        "id": "KBeAglaX0gNW"
      },
      "id": "KBeAglaX0gNW",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "# Take 5000 samples each from the member and non-member sets\n",
        "member_dataset = Subset(full_train, range(5000))\n",
        "nonmember_dataset = Subset(full_test_eval, range(5000))\n"
      ],
      "metadata": {
        "id": "ZBtMbGUM0y_o"
      },
      "id": "ZBtMbGUM0y_o",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loader_test_MIASHIELD_nonmem = DataLoader(nonmember_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "6Or4P0LS12uO"
      },
      "id": "6Or4P0LS12uO",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ece_scores = [compute_ece(m, loader_test_MIASHIELD_nonmem, device) for m in models]\n",
        "print(\"ECE Scores:\", ece_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_6nEX1U14Ee",
        "outputId": "311231b7-97c4-4685-cd5c-38946606dbc9"
      },
      "id": "R_6nEX1U14Ee",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECE Scores: [0.18048515915870667, 0.181722491979599, 0.14850416779518127, 0.196381077170372, 0.1426084339618683]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Confidence Deviation Exclusion (CDE)\n",
        "def extract_confidences_cde(dataset, models, device):\n",
        "    confidences = []\n",
        "    for i in range(len(dataset)):\n",
        "        x_tensor, _ = dataset[i]\n",
        "        pred_probs, _ = confidence_deviation_predict(x_tensor, models, device)\n",
        "        confidence = np.max(pred_probs)\n",
        "        confidences.append(confidence)\n",
        "    return np.array(confidences)\n",
        "\n",
        "# 2. Historical Calibration Error (HCE)\n",
        "def extract_confidences_hce(dataset, models, device, ece_scores):\n",
        "    confidences = []\n",
        "    for i in range(len(dataset)):\n",
        "        x_tensor, _ = dataset[i]\n",
        "        pred_probs, _ = calibration_weighted_predict(x_tensor, models, device, ece_scores=ece_scores)\n",
        "        confidence = np.max(pred_probs)\n",
        "        confidences.append(confidence)\n",
        "    return np.array(confidences)\n",
        "\n",
        "# 3. KL Divergence Exclusion\n",
        "def extract_confidences_kl(dataset, models, device):\n",
        "    confidences = []\n",
        "    for i in range(len(dataset)):\n",
        "        x_tensor, _ = dataset[i]\n",
        "        pred_probs, _ = hybrid_kl_predict(x_tensor, models, device)\n",
        "        confidence = np.max(pred_probs)\n",
        "        confidences.append(confidence)\n",
        "    return np.array(confidences)\n"
      ],
      "metadata": {
        "id": "aJ0GdWWi01KY"
      },
      "id": "aJ0GdWWi01KY",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "def run_confidence_attack(member_confidences, nonmember_confidences, label=\"MCE Variant\"):\n",
        "    X = np.concatenate([member_confidences, nonmember_confidences]).reshape(-1, 1)\n",
        "    y = np.array([1] * len(member_confidences) + [0] * len(nonmember_confidences))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    attack_model = LogisticRegression().fit(X_train, y_train)\n",
        "    y_pred = attack_model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "    attack_adv = abs(auc - 0.5)\n",
        "\n",
        "\n",
        "    print(f\"\\nConfidence-Based MIA Results for {label}:\")\n",
        "    print(f\"   • Attack Accuracy: {acc * 100:.2f}%\")\n",
        "    print(f\"   • AUC Score: {auc:.4f}\")\n",
        "    print(f\"   • Attack Advantage: {attack_adv:.4f}\")\n"
      ],
      "metadata": {
        "id": "qPcQstMj1ccf"
      },
      "id": "qPcQstMj1ccf",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Confidence Deviation (CDE) ---\n",
        "member_conf_cde = extract_confidences_cde(member_dataset, models, device)\n",
        "nonmember_conf_cde = extract_confidences_cde(nonmember_dataset, models, device)\n",
        "run_confidence_attack(member_conf_cde, nonmember_conf_cde, label=\"CDE\")\n",
        "\n",
        "# --- Historical Calibration Error (HCE) ---\n",
        "member_conf_hce = extract_confidences_hce(member_dataset, models, device, ece_scores)\n",
        "nonmember_conf_hce = extract_confidences_hce(nonmember_dataset, models, device, ece_scores)\n",
        "run_confidence_attack(member_conf_hce, nonmember_conf_hce, label=\"HCE\")\n",
        "\n",
        "# --- KL Divergence (KL-MCE) ---\n",
        "member_conf_kl = extract_confidences_kl(member_dataset, models, device)\n",
        "nonmember_conf_kl = extract_confidences_kl(nonmember_dataset, models, device)\n",
        "run_confidence_attack(member_conf_kl, nonmember_conf_kl, label=\"KL Divergence\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8t12HhF1XGQ",
        "outputId": "64509530-1a8d-48bf-fe16-99947d3d10dd"
      },
      "id": "e8t12HhF1XGQ",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confidence-Based MIA Results for CDE:\n",
            "   • Attack Accuracy: 49.23%\n",
            "   • AUC Score: 0.4989\n",
            "   • Attack Advantage: 0.0011\n",
            "\n",
            "Confidence-Based MIA Results for HCE:\n",
            "   • Attack Accuracy: 50.40%\n",
            "   • AUC Score: 0.5098\n",
            "   • Attack Advantage: 0.0098\n",
            "\n",
            "Confidence-Based MIA Results for KL Divergence:\n",
            "   • Attack Accuracy: 48.83%\n",
            "   • AUC Score: 0.4972\n",
            "   • Attack Advantage: 0.0028\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}