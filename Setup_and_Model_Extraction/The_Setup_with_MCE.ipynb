{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2231f126-c2f0-4363-8e37-7bfeb0fe41cd",
      "metadata": {
        "id": "2231f126-c2f0-4363-8e37-7bfeb0fe41cd"
      },
      "source": [
        "## Implementing the Data Preprocessing (Dataset Partitioning) and Model Training for the baseline Model Confidence Based Exclusion (MCE) defense described in the MIAShield paper.\n",
        "* The target dataset is CIFAR-10, and the target model architecture is AlexNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "8f2b601e-46b6-413e-b4ec-f8253879ccd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f2b601e-46b6-413e-b4ec-f8253879ccd2",
        "outputId": "f709dd4c-0a89-4b0a-e341-a5248b441696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "#imports and setup\n",
        "\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split, Subset\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d77963fa-511c-422b-805b-bdcb1ca69ec2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d77963fa-511c-422b-805b-bdcb1ca69ec2",
        "outputId": "cb4294c9-5a67-4e23-e0ec-e32f93ab7aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45b68eb-be91-4e28-86f0-1cfaff3e9e2d",
      "metadata": {
        "id": "b45b68eb-be91-4e28-86f0-1cfaff3e9e2d"
      },
      "source": [
        "## From the paper, original CIFAR-10 dataset has 50,000 training images (Dtrain) and 10,000 test images (Dtest)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d4fc9cf9-585d-4dc0-a0eb-855c8857a8ba",
      "metadata": {
        "id": "d4fc9cf9-585d-4dc0-a0eb-855c8857a8ba"
      },
      "outputs": [],
      "source": [
        "# Define the transformations (normalization is key for training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Normalization parameters for CIFAR-10\n",
        "    # mean for CIFAR-10 = (0.4914, 0.4822, 0.4465)\n",
        "    #std for CIFAR-10  = (0.2023, 0.1994, 0.2010)\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Load original datasets\n",
        "original_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "original_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1PcPadeTbspo",
      "metadata": {
        "id": "1PcPadeTbspo"
      },
      "source": [
        "## Reproducibility, AlexNet (CIFAR-10), and transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c0473744-2170-4d09-b922-d226de729ff5",
      "metadata": {
        "id": "c0473744-2170-4d09-b922-d226de729ff5"
      },
      "outputs": [],
      "source": [
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ---- AlexNet variant for CIFAR-10 (32x32) ----\n",
        "class CIFARAlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # 32x32 -> 32x32\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # 32x32 -> 16x16\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # 16x16 -> 16x16\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # 16x16 -> 8x8\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # 8x8 -> 8x8\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # 8x8 -> 8x8\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # 8x8 -> 8x8\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # 8x8 -> 4x4\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # 256 * 4 * 4 = 4096\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256*4*4, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)  # (N, 256*4*4)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Augmentation for training (paper §6.3): flip, ±10° rotation, ±10% translate, ~0.2% zoom\n",
        "train_transform_w_aug = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.10, 0.10), scale=(0.998, 1.002)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# No-aug (for eval and loaders that shouldn't augment)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uQOr4zJScnJD",
      "metadata": {
        "id": "uQOr4zJScnJD"
      },
      "source": [
        "## Data Partition CIFAR-10 exactly as in Table 1 (n=5) and build EO/MIAShield splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "rm2ovQjnciYf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm2ovQjnciYf",
        "outputId": "93a8a503-b08a-48f7-cad7-c55cb9d247ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Dataset Partition Summary =====\n",
            "Total CIFAR-10 Train Samples: 50000\n",
            "Total CIFAR-10 Test Samples:  10000\n",
            "\n",
            "Subset Dtrain_1: 10000 samples\n",
            "Subset Dtrain_2: 10000 samples\n",
            "Subset Dtrain_3: 10000 samples\n",
            "Subset Dtrain_4: 10000 samples\n",
            "Subset Dtrain_5: 10000 samples\n",
            "\n",
            "--- Exclusion Oracle (EO) Train Set ---\n",
            "Members (from train):      12500\n",
            "Non-members (from test):   5000\n",
            "\n",
            "--- MIAShield Test Set ---\n",
            "Members (from train):      5000\n",
            "Non-members (from test):   5000\n"
          ]
        }
      ],
      "source": [
        "# Use the already-downloaded datasets but re-wrap them with the correct transforms when needed\n",
        "full_train_eval = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=eval_transform)\n",
        "full_test_eval  = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=eval_transform)\n",
        "\n",
        "NUM_TRAIN = len(full_train_eval)   # 50_000\n",
        "NUM_TEST  = len(full_test_eval)    # 10_000\n",
        "n = 5  # number of disjoint subsets (CIFAR-10)\n",
        "\n",
        "# ---- Disjoint split of the training set into n equal parts ----\n",
        "all_train_idx = np.arange(NUM_TRAIN)\n",
        "np.random.shuffle(all_train_idx)\n",
        "splits = np.array_split(all_train_idx, n)  # list of 5 arrays (~10k each)\n",
        "\n",
        "# ---- EO train set: 2.5k x n members (from train) + 5k non-members (from test) ----\n",
        "EO_MEM_PER_SPLIT = 2500\n",
        "eo_mem_indices = []\n",
        "for s in splits:\n",
        "    eo_mem_indices.extend(np.random.choice(s, size=EO_MEM_PER_SPLIT, replace=False))\n",
        "eo_mem_indices = np.array(eo_mem_indices)  # length = 12_500\n",
        "\n",
        "eo_nonmem_indices = np.random.choice(np.arange(NUM_TEST), size=5000, replace=False)\n",
        "\n",
        "Dtrain_EO_members     = Subset(full_train_eval, eo_mem_indices.tolist())\n",
        "Dtrain_EO_nonmembers  = Subset(full_test_eval,  eo_nonmem_indices.tolist())\n",
        "\n",
        "# ---- MIAShield test set: 5k members (from train) + 5k non-members (from test)\n",
        "# ensure disjointness with Dtrain_EO to avoid bias/leakage\n",
        "remaining_train = np.setdiff1d(all_train_idx, eo_mem_indices, assume_unique=False)\n",
        "remaining_test  = np.setdiff1d(np.arange(NUM_TEST), eo_nonmem_indices, assume_unique=False)\n",
        "test_mem_indices    = np.random.choice(remaining_train, size=5000, replace=False)\n",
        "test_nonmem_indices = np.random.choice(remaining_test,  size=5000, replace=False)\n",
        "\n",
        "Dtest_MIASHIELD_members    = Subset(full_train_eval, test_mem_indices.tolist())\n",
        "Dtest_MIASHIELD_nonmembers = Subset(full_test_eval,  test_nonmem_indices.tolist())\n",
        "\n",
        "# === Summary of Dataset Partitioning (CIFAR-10) ===\n",
        "print(\"===== Dataset Partition Summary =====\")\n",
        "print(f\"Total CIFAR-10 Train Samples: {len(full_train_eval)}\")\n",
        "print(f\"Total CIFAR-10 Test Samples:  {len(full_test_eval)}\\n\")\n",
        "\n",
        "for i, s in enumerate(splits, start=1):\n",
        "    print(f\"Subset Dtrain_{i}: {len(s)} samples\")\n",
        "\n",
        "print(\"\\n--- Exclusion Oracle (EO) Train Set ---\")\n",
        "print(f\"Members (from train):      {len(Dtrain_EO_members)}\")\n",
        "print(f\"Non-members (from test):   {len(Dtrain_EO_nonmembers)}\")\n",
        "\n",
        "print(\"\\n--- MIAShield Test Set ---\")\n",
        "print(f\"Members (from train):      {len(Dtest_MIASHIELD_members)}\")\n",
        "print(f\"Non-members (from test):   {len(Dtest_MIASHIELD_nonmembers)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eTKLU3vJdNeX",
      "metadata": {
        "id": "eTKLU3vJdNeX"
      },
      "source": [
        "## DataLoaders (batch size = 128) for training and evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "Sja6CnqcdB5j",
      "metadata": {
        "id": "Sja6CnqcdB5j"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "# For each subset, train loader **with augmentation** and an eval loader **without**.\n",
        "# Build subset Datasets twice: one with aug transform for training, one with eval transform for evaluation.\n",
        "def make_subset_dataset(indices, with_aug: bool):\n",
        "    base = torchvision.datasets.CIFAR10(root='./data', train=True, download=False,\n",
        "                                        transform=train_transform_w_aug if with_aug else eval_transform)\n",
        "    return Subset(base, indices.tolist())\n",
        "\n",
        "subset_train_loaders = []\n",
        "subset_eval_loaders  = []\n",
        "\n",
        "for s in splits:\n",
        "    ds_train = make_subset_dataset(s, with_aug=True)\n",
        "    ds_eval  = make_subset_dataset(s, with_aug=False)\n",
        "    subset_train_loaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True))\n",
        "    subset_eval_loaders.append( DataLoader(ds_eval,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True))\n",
        "\n",
        "# EO & MIAShield loaders (evaluation only — no aug)\n",
        "loader_train_EO_mem    = DataLoader(Dtrain_EO_members,    batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "loader_train_EO_nonmem = DataLoader(Dtrain_EO_nonmembers, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "loader_test_MIASHIELD_mem    = DataLoader(Dtest_MIASHIELD_members,    batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "loader_test_MIASHIELD_nonmem = DataLoader(Dtest_MIASHIELD_nonmembers, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Full test loader for reporting model accuracies (like Table 2)\n",
        "full_test_loader = DataLoader(full_test_eval, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XqwIvigWd4zg",
      "metadata": {
        "id": "XqwIvigWd4zg"
      },
      "source": [
        "## Train loop (Adam Optimizer, Cross Entropy- Loss Function, 60 epochs @ lr=0.01) and evaluation helpers + weight init helper to assist with training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b1rVe7kweN7U",
      "metadata": {
        "id": "b1rVe7kweN7U"
      },
      "outputs": [],
      "source": [
        "# ---- weight init helper (optional) ----\n",
        "def kaiming_init(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "def build_model():\n",
        "    net = CIFARAlexNet(num_classes=10)\n",
        "    net.apply(kaiming_init)\n",
        "    return net\n",
        "\n",
        "def train_one_model(model, train_loader, epochs=60, device=device):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # ↓↓↓ KEY CHANGES ↓↓↓\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 45, 55], gamma=0.1)\n",
        "    # ↑↑↑ KEY CHANGES ↑↑↑\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running += loss.item() * xb.size(0)\n",
        "        scheduler.step()\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - loss: {running/len(train_loader.dataset):.4f}\")\n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(model, data_loader, device=device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for xb, yb in data_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return 100.0 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "buh-bR3TfIuO",
      "metadata": {
        "id": "buh-bR3TfIuO"
      },
      "source": [
        "## Train the five-model ensemble on the five disjoint subsets (with augmentation) and report accuracies (Table 2 style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "CsIzzMTKe9mO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsIzzMTKe9mO",
        "outputId": "178c2b39-2e79-4303-b633-62c31179fc41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training model f1 on its disjoint subset (with augmentation)...\n",
            "Epoch 5/60 - loss: 1.7597\n",
            "Epoch 10/60 - loss: 1.3991\n",
            "Epoch 15/60 - loss: 1.2060\n",
            "Epoch 20/60 - loss: 1.0596\n",
            "Epoch 25/60 - loss: 0.9324\n",
            "Epoch 30/60 - loss: 0.8256\n",
            "Epoch 35/60 - loss: 0.6176\n",
            "Epoch 40/60 - loss: 0.5691\n",
            "Epoch 45/60 - loss: 0.5226\n",
            "Epoch 50/60 - loss: 0.4937\n",
            "Epoch 55/60 - loss: 0.4862\n",
            "Epoch 60/60 - loss: 0.4715\n",
            "f1 accuracy on CIFAR-10 test (with aug): 73.59%\n",
            "Saved model as f1.pth\n",
            "\n",
            "Training model f2 on its disjoint subset (with augmentation)...\n",
            "Epoch 5/60 - loss: 1.6645\n",
            "Epoch 10/60 - loss: 1.3652\n",
            "Epoch 15/60 - loss: 1.1896\n",
            "Epoch 20/60 - loss: 1.0478\n",
            "Epoch 25/60 - loss: 0.9410\n",
            "Epoch 30/60 - loss: 0.8215\n",
            "Epoch 35/60 - loss: 0.6033\n",
            "Epoch 40/60 - loss: 0.5554\n",
            "Epoch 45/60 - loss: 0.5113\n",
            "Epoch 50/60 - loss: 0.4750\n",
            "Epoch 55/60 - loss: 0.4731\n",
            "Epoch 60/60 - loss: 0.4638\n",
            "f2 accuracy on CIFAR-10 test (with aug): 73.53%\n",
            "Saved model as f2.pth\n",
            "\n",
            "Training model f3 on its disjoint subset (with augmentation)...\n",
            "Epoch 5/60 - loss: 1.6357\n",
            "Epoch 10/60 - loss: 1.3417\n",
            "Epoch 15/60 - loss: 1.1523\n",
            "Epoch 20/60 - loss: 1.0022\n",
            "Epoch 25/60 - loss: 0.9229\n",
            "Epoch 30/60 - loss: 0.8256\n",
            "Epoch 35/60 - loss: 0.5995\n",
            "Epoch 40/60 - loss: 0.5388\n",
            "Epoch 45/60 - loss: 0.5103\n",
            "Epoch 50/60 - loss: 0.4633\n",
            "Epoch 55/60 - loss: 0.4672\n",
            "Epoch 60/60 - loss: 0.4532\n",
            "f3 accuracy on CIFAR-10 test (with aug): 74.19%\n",
            "Saved model as f3.pth\n",
            "\n",
            "Training model f4 on its disjoint subset (with augmentation)...\n",
            "Epoch 5/60 - loss: 1.5888\n",
            "Epoch 10/60 - loss: 1.3083\n",
            "Epoch 15/60 - loss: 1.1017\n",
            "Epoch 20/60 - loss: 0.9565\n",
            "Epoch 25/60 - loss: 0.8522\n",
            "Epoch 30/60 - loss: 0.7403\n",
            "Epoch 35/60 - loss: 0.5140\n",
            "Epoch 40/60 - loss: 0.4780\n",
            "Epoch 45/60 - loss: 0.4415\n",
            "Epoch 50/60 - loss: 0.4004\n",
            "Epoch 55/60 - loss: 0.3933\n",
            "Epoch 60/60 - loss: 0.3977\n",
            "f4 accuracy on CIFAR-10 test (with aug): 74.92%\n",
            "Saved model as f4.pth\n",
            "\n",
            "Training model f5 on its disjoint subset (with augmentation)...\n",
            "Epoch 5/60 - loss: 1.7054\n",
            "Epoch 10/60 - loss: 1.3872\n",
            "Epoch 15/60 - loss: 1.1908\n",
            "Epoch 20/60 - loss: 1.0493\n",
            "Epoch 25/60 - loss: 0.9325\n",
            "Epoch 30/60 - loss: 0.8088\n",
            "Epoch 35/60 - loss: 0.5807\n",
            "Epoch 40/60 - loss: 0.5362\n",
            "Epoch 45/60 - loss: 0.4954\n",
            "Epoch 50/60 - loss: 0.4662\n",
            "Epoch 55/60 - loss: 0.4527\n",
            "Epoch 60/60 - loss: 0.4585\n",
            "f5 accuracy on CIFAR-10 test (with aug): 75.21%\n",
            "Saved model as f5.pth\n"
          ]
        }
      ],
      "source": [
        "models = []\n",
        "acc_table = []\n",
        "for i, tr_loader in enumerate(subset_train_loaders, start=1):\n",
        "    print(f\"\\nTraining model f{i} on its disjoint subset (with augmentation)...\")\n",
        "    net = build_model()\n",
        "    net = train_one_model(net, tr_loader, epochs=60)\n",
        "\n",
        "    # Store in-memory\n",
        "    models.append(net)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    acc = accuracy(net, full_test_loader)\n",
        "    acc_table.append((f\"f{i}\", acc))\n",
        "    print(f\"f{i} accuracy on CIFAR-10 test (with aug): {acc:.2f}%\")\n",
        "\n",
        "     # ---- SAVE TO DISK ----\n",
        "    torch.save(net.state_dict(), f\"f{i}.pth\")\n",
        "    print(f\"Saved model as f{i}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "TxtgQFpHXHJ6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxtgQFpHXHJ6",
        "outputId": "b8fdd033-944e-48df-df70-cbed5b7b5234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zip created!\n"
          ]
        }
      ],
      "source": [
        "## If you want to download it to your system - Run the 2 code block below\n",
        "## Or Skip and Load it from Step 2\n",
        "\n",
        "#import zipfile\n",
        "\n",
        "#model_files = [\"f1.pth\", \"f2.pth\", \"f3.pth\", \"f4.pth\", \"f5.pth\"]\n",
        "\n",
        "#with zipfile.ZipFile(\"models_f1_f5.zip\", 'w') as zipf:\n",
        " #   for file in model_files:\n",
        "  #      zipf.write(file)\n",
        "\n",
        "#print(\"Zip created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "XNBcDRwzcira",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XNBcDRwzcira",
        "outputId": "b1d619b5-74a0-4171-8bcd-4c8427ae6eee"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_d5e80256-1b04-42ea-9cc8-884c3995d5ff\", \"models_f1_f5.zip\", 717130407)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#from google.colab import files\n",
        "#files.download(\"models_f1_f5.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gDqeLLlbdGOx",
      "metadata": {
        "id": "gDqeLLlbdGOx"
      },
      "source": [
        "## For Step 2\n",
        "\n",
        "Load the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3L3cgsnWdgRQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L3cgsnWdgRQ",
        "outputId": "db8da8d3-3b90-4d9c-81a4-827c898f226a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1 test accuracy: 73.59\n"
          ]
        }
      ],
      "source": [
        "def load_model(path):\n",
        "    model = build_model().to(device)\n",
        "    state_dict = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Load the five models\n",
        "f1 = load_model(\"f1.pth\")\n",
        "f2 = load_model(\"f2.pth\")\n",
        "f3 = load_model(\"f3.pth\")\n",
        "f4 = load_model(\"f4.pth\")\n",
        "f5 = load_model(\"f5.pth\")\n",
        "\n",
        "models = [f1, f2, f3, f4, f5]\n",
        "\n",
        "# (Optional) double-check f1 accuracy achieved in the setup\n",
        "print(\"f1 test accuracy:\", accuracy(f1, full_test_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08d885d",
      "metadata": {},
      "source": [
        "APPLYING MCE TO THE CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db86f2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def mce_oracle_predict(x_tensor, ensemble_models, device):\n",
        "    all_predictions = []\n",
        "    for model in ensemble_models:\n",
        "        model.eval() # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            output = model(x_tensor.unsqueeze(0).to(device))\n",
        "            # Get probabilities\n",
        "            probabilities = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
        "            all_predictions.append(probabilities)\n",
        "\n",
        "    predictions = np.array(all_predictions) # Shape: (num_models, num_classes)\n",
        "\n",
        "    top_labels = [np.argmax(p) for p in predictions]\n",
        "\n",
        "    # Handle cases where top_labels might be empty\n",
        "    if not top_labels:\n",
        "        return np.zeros(predictions.shape[1]), 0\n",
        "\n",
        "    # Find the majority label among the top predictions of all models\n",
        "    majority_label_counts = Counter(top_labels)\n",
        "    majority_label = majority_label_counts.most_common(1)[0][0]\n",
        "\n",
        "    # Calculate confidence of each model in the majority label\n",
        "    label_confidences = [p[majority_label] for p in predictions]\n",
        "    # Exclude the model with the *lowest* confidence in the majority class\n",
        "    excluded_idx = np.argmax(label_confidences)\n",
        "\n",
        "    # Remove the predictions of the excluded model and average the rest\n",
        "    remaining_preds = np.delete(predictions, excluded_idx, axis=0)\n",
        "    final_pred = np.mean(remaining_preds, axis=0)\n",
        "\n",
        "    return final_pred, excluded_idx\n",
        "\n",
        "def evaluate_mce_on_test(dataset, models, device):\n",
        "    correct = 0\n",
        "    exclusion_counts = np.zeros(len(models), dtype=int)\n",
        "    total_samples = len(dataset)\n",
        "\n",
        "    print(f\"\\nEvaluating MCE Oracle on {total_samples} test samples...\")\n",
        "    for i in range(total_samples):\n",
        "        x_tensor, y_true = dataset[i]\n",
        "\n",
        "        pred_probs, excluded = mce_oracle_predict(x_tensor, models, device)\n",
        "        y_pred = np.argmax(pred_probs)\n",
        "\n",
        "        if y_pred == y_true:\n",
        "            correct += 1\n",
        "\n",
        "        exclusion_counts[excluded] += 1\n",
        "\n",
        "    accuracy = correct / total_samples\n",
        "    print(f\"\\nMCE Oracle Accuracy on full test set: {accuracy * 100:.2f}%\")\n",
        "    print(\"Model Exclusion Counts:\")\n",
        "    for i, count in enumerate(exclusion_counts):\n",
        "        print(f\"   • Model {i+1}: excluded {count} times\")\n",
        "\n",
        "    return accuracy, exclusion_counts\n",
        "\n",
        "\n",
        "acc_mce, exclusion_stats = evaluate_mce_on_test(full_test_eval, models, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
